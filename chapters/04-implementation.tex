\chapter{本提案手法の実装}\label{cha:Implementation}
本提案手法は、領域座標取得部、文字情報取得部、ラベル付与部の3つの処理部で構成する。
本提案手法の構成を、図\ref{fig:structure}に示す。
以降、本章では3つの処理部について説明する。

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=15cm]{image/structure.jpg}
        \caption{本提案手法の構造}
        \label{fig:structure}
    \end{center}
\end{figure}


\section{領域座標取得部}\label{sec:area_coords_obtainment_part}
領域座標取得部は、帳票画像内記入欄を検出し、取得した座標を領域座標として出力する。
矩形の帳票画像記入欄については、各頂点のxy座標を、下線部の帳票画像記入欄については、両端点のxy座標を領域座標として取得する。
領域座標取得部の出力結果である領域座標は、ラベル付与部(\ref{subsec:label_link_processing}節で後述)で用いる。


\subsection{矩形領域座標取得処理}\label{subsec:rect_coords_obtainment_processing}
矩形領域座標取得処理は、矩形の記入欄を検出し、各頂点のxy座標を矩形領域座標として取得し、出力する処理である。
本処理の出力は、下線部領域座標取得処理(\ref{subsec:underline_coords_obtainment_processing})の一部で利用する。
矩形の取得にあたり、処理画像は白または黒でなければならないため、帳票画像に画像処理を施す必要がある。
以下に矩形領域座標の取得に必要な画像処理の順を示す。

\begin{enumerate}
    \item OpenCVのcvtColor関数を用いた、帳票画像のグレースケール化
    \item DeblurGANv2(\ref{sec:DeblurGANv2}節を参照)の適用によるブレ除去後のグレースケール化帳票画像の生成\\
        DeblurGANv2を適用することにより、スマートフォンで帳票画像を撮影する際に発生する画像内のブレを除去し、矩形の検出精度を高める。
    \item OpenCVのGaussianBlur関数(\ref{sec:OpenCV}節を参照)を用いた、ガウシアンフィルタによるノイズ除去\\
        本提案手法では、カーネルの縦幅と横幅を共に3、標準偏差を0とする。
    \item OpenCVのthreshold関数(\ref{sec:OpenCV}節を参照)を用いた、大津の二値化による二値画像への変換\\
        本提案手法では、閾値処理を大津の二値化としたTHRESH\_TOZERO\_INVによる二値化手法とする(\ref{sec:OpenCV}節を参照)。
        白黒を反転して二値化することにより、複数の矩形が隣接する場合に、それらを外接する最小の外接矩形を不要に検出してしまうことを防ぐ。
    \item OpenCVのgetStructuringElement関数(\ref{sec:OpenCV}節を参照)を用いた、カーネルの作成\\
        本提案手法では、5行5列の矩形カーネルを作成する。
\end{enumerate}

スマートフォンのカメラで撮影する際にブレが生じた帳票画像を図\ref{fig:before_deblur}に示す。
図\ref{fig:before_deblur}に対して、DeblurGANv2を適用することによってブレを除去した画像を図\ref{fig:after_deblur}に示す。
図\ref{fig:before_deblur}と図\ref{fig:after_deblur}より、DeblurGANv2によるブレ除去によって、図\ref{fig:before_deblur}でブレている矩形の線が図\ref{fig:after_deblur}ではっきりとなっていることがわかる。

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.45\linewidth}
      \centering
      \fbox{
        \includegraphics[keepaspectratio, width=7cm]{image/04-implementation/before_deblur.png}
      }
      \caption{撮影するにあたってブレが生じた帳票画像}
      \label{fig:before_deblur}
    \end{minipage}
    \begin{minipage}[t]{0.45\linewidth}
      \centering
      \fbox{
        \includegraphics[keepaspectratio, width=7cm]{image/04-implementation/after_deblur.jpeg}
      }
      \caption{発生したブレを除去した帳票画像}
      \label{fig:after_deblur}
    \end{minipage}
\end{figure}

画像処理後、OpenCVのfindContours関数(\ref{sec:OpenCV}節を参照)による輪郭検出を用いて矩形の帳票画像記入欄を検出し、矩形領域座標を取得する。
なお、以下の条件のいずれかに該当する矩形については、記入欄として不適切であり誤検知の場合があるとして、出力の対象外とする。

\begin{itemize}
    \item 面積が3000ピクセル以下である場合
    \item ある辺の長さが10ピクセル以下である場合
\end{itemize}

DeblurGANv2によるブレ除去を行っていない画像を示す図\ref{fig:before_deblur}に対して、矩形領域座標取得処理の出力結果を描画した画像を図\ref{fig:before_deblur_area}に示す。
DeblurGANv2によるブレ除去を行った画像を示す図\ref{fig:after_deblur}に対し、DeblurGANv2を適用することによってブレを除去した画像を入力として、矩形領域座標取得処理の出力結果を描画した画像を図\ref{fig:after_deblur_area}に示す。
図\ref{fig:before_deblur_area}と図\ref{fig:after_deblur_area}より、DeblurGANv2によるブレ除去によって、矩形領域の検出精度が向上していることがわかる。

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.45\linewidth}
      \centering
      \fbox{
        \includegraphics[keepaspectratio, width=7cm]{image/04-implementation/before_deblur_area.jpeg}
      }
      \caption{DeblurGANv2を適用していないブレのある帳票画像の領域座標取得処理の出力}
      \label{fig:before_deblur_area}
    \end{minipage}
    \begin{minipage}[t]{0.45\linewidth}
      \centering
      \fbox{
        \includegraphics[keepaspectratio, width=7cm]{image/04-implementation/after_deblur_area.jpeg}
      }
      \caption{DeblurGANv2適用後のブレを除去した帳票画像の領域座標取得処理の出力}
      \label{fig:after_deblur_area}
    \end{minipage}
\end{figure}




\subsection{下線部領域座標取得処理}\label{subsec:underline_coords_obtainment_processing}
下線部領域座標取得処理は、下線部の記入欄を検出し、両端点のxy座標を下線部領域座標として取得し、出力する処理である。
下線部の取得にあたり、処理画像は白または黒でなければならないため、帳票画像に画像処理を施す必要がある。
以下に下線部領域座標の取得に必要な画像処理の順を示す。
なお、本処理における画像処理の一部は、矩形領域座標取得処理(\ref{subsec:rect_coords_obtainment_processing}節)と同様の画像処理を施す。

\begin{enumerate}
    \item OpenCVのcvtColor関数を用いた、帳票画像のグレースケール化\\
        \ref{subsec:rect_coords_obtainment_processing}節と同様の処理
    \item DeblurGANv2の適用によるブレ除去後のグレースケール化帳票画像の生成\\
        \ref{subsec:rect_coords_obtainment_processing}節と同様の処理
    \item OpenCVのthreshold関数を用いた、大津の二値化による二値画像への変換\\
        本提案手法では、閾値処理を大津の二値化としたTHRESH\_BINARYによる二値化手法とする。
    \item OpenCVのthreshold関数(\ref{sec:OpenCV}節を参照)を用いた、Canny法によるエッジ検出\\
        本提案手法では、閾値処理における上限と下限の閾値を、以下のように決定する。
        \begin{enumerate}
            \item 構成画素の画素値の中央値を取得する。
            \item 中央値の定数倍(本研究では定数を0.33とする)を、取得した中央値から加減算する。
            \item 加算した値を上限の閾値、減算した値を下限の閾値として設定する。
        \end{enumerate}
\end{enumerate}

画像処理後、OpenCVのHoughLinesP関数(\ref{sec:OpenCV}節を参照)によるハフ変換を用いて下線の帳票画像記入欄を検出し、下線部領域座標を取得する。
なお、以下の条件のいずれかに該当する直線については、記入欄として不適切であり誤検出の場合があるとして、出力の対象外とする。
条件の1つに矩形領域座標取得処理(\ref{subsec:rect_coords_obtainment_processing}節)の出力を利用する。

\begin{itemize}
    \item 直線の長さが10ピクセル未満である場合
        Canny法で適用するガウシアンフィルタで除去できていないノイズによって誤検出したエッジを下線と捉えることを防ぐ。
    \item 水平を基準として傾きが3ピクセル以上である場合
        横書きの帳票において、下線部の直線は水平であるため、垂直な直線を下線と捉えることを防ぐ。
    \item 直線が矩形領域の辺の一部から上下20ピクセル以内に存在する場合\\
        矩形領域座標取得処理の出力を利用する。矩形領域の辺の一部を下線と捉えることを防ぐ。
\end{itemize}

エッジ検出を施すことにより、直線の検出精度が向上するが、直線の上下に2本の直線を検出してしまう場合がある。
これに対しては、検出した直線の中点を全て計算し、ある直線における中点のy座標について、上下10ピクセル以内に別の直線の中点が存在する場合は、二直線の両端点のxy座標をそれぞれ平均して一本の直線に統一することによって不具合を解消する。

\section{文字情報取得部}\label{sec:OCR_part}
文字情報取得部では、光学文字認識(\ref{sec:Optical-Charactor-Recognition}節を参照)によって、帳票画像内の文字についての情報を取得する。
本提案手法では、光学文字認識ソフトTesseract-OCRを用いて、文字と、文字を囲うバウンディングボックスの各頂点のxy座標を文字位置として取得する。
文字情報取得部の出力結果は、ラベル付与部(\ref{subsec:label_link_processing}節で後述)で用いる。

\subsection{文字認識処理}\label{subsec:char_recognition_processing}
文字認識処理は、認識した文字を取得文字として取得する処理である。
文字の取得にあたり、文字の認識精度を高めるため、以下の順で帳票画像に画像処理を施す。

\begin{enumerate}
    \item DeblurGANv2の適用によるブレ除去後のグレースケール化帳票画像の生成\\
        \ref{subsec:rect_coords_obtainment_processing}節と同様の処理
    \item OpenCVのcvtColor関数を用いた、帳票画像のグレースケール化\\
        \ref{subsec:rect_coords_obtainment_processing}節と同様の処理
    \item OpenCVのthreshold関数を用いた、大津の二値化による二値画像への変換\\
        本提案手法では、閾値処理を大津の二値化としたTHRESH\_BINARYによる二値化手法とする。
\end{enumerate}

画像処理後、Tesseract-OCRによる文字認識を行う。文字認識処理と同時に、文字認識処理(\ref{subsec:char_recognition_processing}節)を行う。


\subsection{文字位置取得処理}\label{subsec:char_position_obtainment_processing}
文字位置取得処理は、認識した文字を囲うバウンディングボックスの各頂点のxy座標を文字位置として取得する処理である。
本処理は文字認識処理(\ref{subsec:char_recognition_processing}節)と同時に行い、文字位置を取得する。

文字位置取得後、バウンディングボックスの左上の頂点座標について、y座標の値が小さい順に文字認識処理(\ref{subsec:char_recognition_processing}節)で取得した文字と組とし、番号を割り振る。y座標が同じ場合は、さらにx座標が小さい順にソートする。
しかし、人間の目視で複数の文字列が同じ行に存在すると認識するとき、左右の順番がバラバラとなる場合がある。
これは、Tesseract-OCRが文字を認識する順番を、y座標についてピクセル単位で降順にソートするため、人間の目視で認識する順番とソート後の順番に違いが生じるためである。
ラベル付与部(\ref{subsec:label_link_processing}節で後述)で文字位置を扱う際に、ソート結果が人間の認識と異なる場合、ラベルの更新順が変化してしまうため、意図しないラベルを領域座標に割り付ける不具合が発生する場合がある。
この不具合の発生を防ぐため、ソートで比較する値は、y座標を10分の1にした値(小数点以下切り捨て)を用いてソートを行う。

ある帳票画像に対して文字位置取得処理を施し、同一行における取得文字の左右の順番をバラバラに認識した場合のバウンディングボックスの画像を図\ref{fig:before_sorted_string}に示す。
図\ref{fig:before_sorted_string}内の12番から15番について、左から14番、15番、13番、12番となっている。
これに対し、y座標を10分の1にした値で比較することにより、左右の順番を左から右へ番号が大きくなるようソートした場合のバウンディングボックスの画像を図\ref{fig:after_sorted_string}に示す。
ソート後は、図\ref{fig:after_sorted_string}内における12番から15番は、左から12番、13番、14番、15番となっており、左右の順番を入れ替えることに成功していることがわかる。

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=15cm]{image/04-implementation/before_sorted_string.png}
        \caption{左右の順番をバラバラに認識した文字位置取得処理の出力}
        \label{fig:before_sorted_string}
    \end{center}
\end{figure}

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=15cm]{image/04-implementation/after_sorted_string.png}
        \caption{同行の番号を左から右へ昇順となるようソートした文字位置取得処理の出力}
        \label{fig:after_sorted_string}
    \end{center}
\end{figure}


\subsection{除外判定処理}\label{subsec:exclusion_judgement_processing}
除外判定処理は、Fugashi(\ref{sec:Fugashi}節を参照)による形態素解析を行い、属性判定処理(\ref{subsec:att_prediction_processing}節で後述)に不要である取得文字について、出力から除外する処理である。
形態素の品詞を解析し、取得文字の構成形態素数のうち、特定の品詞である形態素数の割合が半分以上である場合は、属性判定において意味がない取得文字であるとして、該当の取得文字と文字位置を文字情報取得部の出力から除外する。
文字を認識する際に、紙面と背景の境界や、矩形や直線を文字として誤認識する場合がある。
不要な文字の属性推測を防ぐことにより、処理時間を短縮することが可能である。

除外対象である形態素の品詞を、UniDic品詞体系(左からカンマ区切りで、大分類、中分類、小分類、細分類)をもとに以下に示す。
なお、除外対象とする品詞は経験から決定している。

\begin{itemize}
    \item 補助記号,一般,*,*
    \item 感動詞,フィラー,*,*
\end{itemize}

ある帳票画像に対して、認識した文字のバウンディングボックスを描画した画像の一部を図\ref{fig:before_exclusion_bbox}に示す。
図\ref{fig:before_exclusion_bbox}の文字認識の結果の画像を図\ref{fig:before_exclusion_string}に示す。
図\ref{fig:before_exclusion_bbox}、図\ref{fig:before_exclusion_string}内の58番および60番は、帳票画像内の矩形を誤って文字として認識しており、取得文字は属性判定において意味がない文字である。
図\ref{fig:before_exclusion_string}に対して除外判定処理を適用し、属性判定に不要な取得文字を除外した出力を図\ref{fig:after_exclusion_string}に示す。
図\ref{fig:after_exclusion_string}より、属性判定に不要な取得文字の除外に成功していることがわかる。

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=15cm]{image/04-implementation/before_exclusion_bbox.png}
        \caption{属性判定に不要な文字を含む文字認識}
        \label{fig:before_exclusion_bbox}
    \end{center}
\end{figure}

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=15cm]{image/04-implementation/before_exclusion_string.png}
        \caption{除外判定処理適用前の取得文字}
        \label{fig:before_exclusion_string}
    \end{center}
\end{figure}

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=15cm]{image/04-implementation/after_exclusion_string.png}
        \caption{除外判定処理適用後の取得文字}
        \label{fig:after_exclusion_string}
    \end{center}
\end{figure}


\section{ラベル付与部}\label{sec:label_link_part}
ラベル付与部では、除外判定処理(\ref{subsec:exclusion_judgement_processing}節)後の取得文字の属性を日付(date)、文字列(string)、数値(number)の3つから適当なものを推測し、取得した領域にラベルとして付与する。
付与するラベルの種類は、領域近傍の取得文字から推測する属性に依存する。
領域座標と、領域座標に対応するラベルを組としたJSON形式のファイルを出力とする。


\subsection{属性推測処理}\label{subsec:att_prediction_processing}
属性推測処理では、取得文字に対して、日付(date)、文字列(string)、数値(number)の3つの属性のいずれに該当するかを推測する。属性が推測不可である取得文字は、文字列として属性を推測する。
属性の推測には、大規模言語モデルYouri(\ref{sec:Youri}節を参照)を用いる。YouriはLlama2を日本語の学習データで継続事前学習を行った大規模言語モデルである。
大規模言語モデルは、指示と異なる出力をする場合があり、Youriの出力をそのまま属性とすると、候補ではない属性の推測によってラベル割付処理(\ref{subsec:label_link_processing}で後述)で意図しないラベルを割り付ける場合がある。
候補ではない属性の推測を防ぐため、Youriの出力から3つの属性のいずれかとなるよう補正を行う。
日本語の推論に特化した言語モデルを利用することによって、取得した日本語の文字に属性をより正確に推測することが可能となる。

属性を推測する流れを以下に示す。
以下の処理は、文字位置取得処理(\ref{subsec:char_position_obtainment_processing}節)でソートを行った順番で、除外判定処理(\ref{subsec:exclusion_judgement_processing}節)後の取得文字の数だけ繰り返す。

\begin{enumerate}
    \item 除外判定処理(\ref{subsec:exclusion_judgement_processing}節)の出力である除外判定後の取得文字を受け取る。
    \item 以下のプロンプトを入力として属性を推測する。\\
        書類の項目として、(取得文字)という欄に記入する内容がどのデータ型に該当するかを、日付、文字列、数値の中から最も適切なものを選べ。
    \item 出力結果の文字列に取得文字と同じ文字を含む場合は、出力結果から取得文字のみを削除する。\\
        出力の最大トークン数を50トークンに制限することにより、属性判定ではない出力(単語の説明や類語の出力など)を防ぐ。
        属性判定ではない出力は、経験から出力に取得文字と同じ文字を含む傾向にある。
        後述する属性の補正を行うにあたり、取得文字に含む文字から補正を行い、意図しない属性に補正することを防ぐ。
    \item 以下の条件分岐により、属性を補正する。
        \begin{itemize}
            \item 初期値として、全文字の属性を文字列(string)とする。
            \item 出力に「日」を含む場合は、日付(date)として判定し、属性を更新する。
            \item 出力に「数」を含む場合は、数値(number)として判定し、属性を更新する。
            \item 出力に「日」、「数」を含まない、または属性が推測不可である場合は、初期値から変化せず、文字列(string)として判定する。
        \end{itemize}
\end{enumerate}



\subsection{ラベル割付処理}\label{subsec:label_link_processing}
ラベル割付処理では、領域座標取得部(\ref{sec:area_coords_obtainment_part}節)で取得した領域座標に対して、近傍に存在する文字の属性を割り付ける。
属性推測処理(\ref{subsec:att_prediction_processing}節)で推測した属性と、文字位置取得処理(\ref{subsec:char_position_obtainment_processing}節)で取得した文字位置をもとに、取得文字近傍の領域座標に対して、推測した属性を割り付ける。

領域座標にラベルを割り付ける流れを以下に示す。
以下の処理は、文字位置取得処理(\ref{subsec:char_position_obtainment_processing}節)でソートを行った順番で、除外判定処理(\ref{subsec:exclusion_judgement_processing}節)後の取得文字の数だけ繰り返す。

\begin{enumerate}
    \item 文字位置であるバウンディングボックスの中心点のxy座標を計算する。
    \item 取得した領域座標のうち、矩形領域は右下にある頂点のxy座標、下線部領域は右端点のxy座標と比較し、計算した中心点のx座標とy座標が共に大きい全ての領域座標に対して、文字位置に対応する取得文字の属性を割り付ける。
    \item 既にラベルを割り付けた領域座標は、ラベルを更新する。
\end{enumerate}

以上の繰り返し処理後、領域座標取得部(\ref{sec:area_coords_obtainment_part}節)で取得した領域座標と、領域座標に対応するラベルの組をJSON形式で出力する。
