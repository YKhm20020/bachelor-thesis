\chapter{本ツールの実装}\label{cha:Implementation}
本ツールは、領域座標取得部、文字情報取得部、ラベル付与部、ファイル出力部の4つの処理部で構成する。
本ツールの構成を、図\ref{fig:structure}に示す。

以降、本章では4つの処理部について説明する。

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=15cm]{image/04-implementation/structure.jpg}
        \caption{本ツールの構造}
        \label{fig:structure}
    \end{center}
\end{figure}


\section{領域座標取得部}\label{sec:area_coords_obtainment_part}
領域座標取得部は、帳票画像内にある記入欄を検出し、領域座標を取得、および、出力する。
矩形の帳票画像記入欄については、各頂点の4つのxy座標を、下線部の帳票画像記入欄については、両端点の2つのxy座標を領域座標として取得する。
領域座標取得部の出力結果である領域座標は、ラベル付与部(\ref{subsec:label_link_processing}節で後述)で用いる。

\subsection{矩形領域座標取得用画像処理}\label{subsec:image_processing_for_rect_coords_obtainment}
矩形領域座標取得用画像処理は、この後の処理である、矩形領域座標取得処理(\ref{subsec:rect_coords_obtainment_processing}節で後述)を実行するための前処理として、画像処理を行う。
本ツールでは、矩形の取得にあたり、OpenCVのfindContours関数(\ref{sec:OpenCV}節を参照)を用いる。
この関数を呼び出すとき、第一引数に渡す処理画像のパスについて、渡したパスの処理画像は白または黒色でなければエラーが発生してしまうため、処理画像を二値化する必要がある。
また、矩形の検出精度を高めるため、ノイズ除去(同節で後述)などの画像処理を組み合わせ、二値化する。

以下に、本処理で施す画像処理を順に示す。

\begin{enumerate}
    \item OpenCVのcvtColor関数を用いた、帳票画像のグレースケール化
        グレースケール化によって、色情報を削減し、計算量を減らすことで処理を高速化する。
    \item DeblurGANv2(\ref{sec:DeblurGANv2}節を参照)の適用によるブレ除去後のグレースケール化帳票画像の生成\\
        DeblurGANv2を適用することにより、帳票画像を撮影する際に発生する画像内のブレを除去し、矩形の検出精度を高める。
    \item OpenCVのGaussianBlur関数(\ref{sec:OpenCV}節を参照)を用いたガウシアンフィルタによるノイズ除去\\
        画像内のノイズを除去し、ノイズを矩形として検出することを防ぐ。
        なお、3行3列の矩形カーネルを使用し、標準偏差を0とする。
    \item OpenCVのthreshold関数(\ref{sec:OpenCV}節を参照)を用いた大津の二値化による二値画像への変換\\
        大津の二値化による閾値処理を行い、閾値の決定手法を指定する変数をTHRESH\_TOZERO\_INVに指定し、二値化する(\ref{sec:OpenCV}節を参照)。
        白黒を反転して二値化することにより、複数の矩形が隣接する場合に、それらを囲む矩形を不要に検出してしまうことを防ぐ。
    \item OpenCVのgetStructuringElement関数(\ref{sec:OpenCV}節を参照)を用いたカーネルの作成\\
        5行5列の矩形カーネルを作成する。
    \item OpenCVのdilate関数(\ref{sec:OpenCV}節を参照)を用いた膨張処理\\
        getStructuringElement関数で作成した矩形カーネルを用いて、。
        なお、膨張する回数は1回とする。
\end{enumerate}


\subsection{矩形領域座標取得処理}\label{subsec:rect_coords_obtainment_processing}
矩形領域座標取得処理は、矩形の帳票画像記入欄を検出し、矩形領域座標を取得、および、出力する処理である。
本処理の出力は、下線部領域座標取得処理(\ref{subsec:underline_coords_obtainment_processing}節で後述)の一部で利用する。

\ref{subsec:image_processing_for_rect_coords_obtainment}節で述べた画像処理後、OpenCVのfindContours関数(\ref{sec:OpenCV}節を参照)による輪郭検出を用いて矩形の帳票画像記入欄を検出する。
なお、以下の条件のいずれかに該当する矩形については、誤検知の可能性が高いとして、出力の対象外とする。

\begin{itemize}
    \item 面積が3000ピクセル以下である場合
    \item 一辺の長さが10ピクセル以下である場合
\end{itemize}



% スマートフォンのカメラで撮影する際にブレが生じた帳票画像を図\ref{fig:before_deblur}に示す。
% 図\ref{fig:before_deblur}に対して、DeblurGANv2を適用することによってブレを除去した画像を図\ref{fig:after_deblur}に示す。
% 図\ref{fig:before_deblur}と図\ref{fig:after_deblur}より、DeblurGANv2によるブレ除去によって、図\ref{fig:before_deblur}でブレている矩形の線が図\ref{fig:after_deblur}ではっきりとなっていることがわかる。

% \begin{figure}[t]
%     \centering
%     \begin{minipage}[t]{0.45\linewidth}
%       \centering
%       \fbox{
%         \includegraphics[keepaspectratio, width=7cm]{image/04-implementation/before_deblur.png}
%       }
%       \caption{撮影するにあたってブレが生じた帳票画像}
%       \label{fig:before_deblur}
%     \end{minipage}
%     \begin{minipage}[t]{0.45\linewidth}
%       \centering
%       \fbox{
%         \includegraphics[keepaspectratio, width=7cm]{image/04-implementation/after_deblur.jpeg}
%       }
%       \caption{発生したブレを除去した帳票画像}
%       \label{fig:after_deblur}
%     \end{minipage}
% \end{figure}


\subsection{下線部領域座標取得用画像処理}\label{subsec:image_processing_for_underline_coords_obtainment}
下線部領域座標取得用画像処理は、この後の処理である下線部領域座標取得処理(\ref{subsec:underline_coords_obtainment_processing}節で後述)を実行するための前処理として、画像処理を行う。
本ツールでは、下線部の取得にあたり、OpenCVのHoughLinesP関数(\ref{sec:OpenCV}節を参照)による、ハフ変換を用いた直線検出を行う。
この関数の第一引数に渡す処理画像のパスについて、渡すパスの処理画像は白または黒色でなければエラーが発生してしまうため、処理画像を二値化する必要がある。
また、直線の検出精度を高めるため、ノイズ除去などの画像処理を組み合わせ、二値化する。

以下に、本処理で施す画像処理を順に示す。
なお、本処理における画像処理の一部は、矩形領域座標取得処理(\ref{subsec:rect_coords_obtainment_processing}節を参照)と同様の画像処理を施す。

\begin{enumerate}
    \item OpenCVのcvtColor関数を用いた帳票画像のグレースケール化\\
        \ref{subsec:rect_coords_obtainment_processing}節で述べた処理と同様の処理
    \item DeblurGANv2の適用によるブレ除去後のグレースケール化帳票画像の生成\\
        \ref{subsec:rect_coords_obtainment_processing}節で述べた処理と同様の処理
    \item OpenCVのthreshold関数を用いた大津の二値化による二値画像への変換\\
        大津の二値化による閾値処理を行い、閾値の決定手法を指定する変数をTHRESH\_BINARY\_INVに指定し、二値化する。
        ハフ変換は白線を検出する処理であるため、白黒を反転して二値化することにより、黒色の直線を白色とし、検出精度を高める。
    \item OpenCVのCanny関数(\ref{sec:OpenCV}節を参照)を用いたCanny法によるエッジ検出\\
        閾値処理における上限と下限の閾値を、以下のように決定する。
        \begin{enumerate}
            \item 大津の二値化で取得した閾値を受け取る。
            \item 二値画像を対象に、画素値の中央値を定数倍(本研究では定数を0.33とする)し、取得した閾値から加減算する。
            \item 加算した値を上限の閾値、減算した値を下限の閾値として設定する。
        \end{enumerate}
\end{enumerate}


\subsection{下線部領域座標取得処理}\label{subsec:underline_coords_obtainment_processing}
下線部領域座標取得処理は、下線部の帳票画像記入欄を検出し、下線部領域座標を取得、および、出力する処理である。

\ref{subsec:image_processing_for_underline_coords_obtainment}節の画像処理後、OpenCVのHoughLinesP関数によるハフ変換を用いて、下線部の帳票画像記入欄を検出する。
なお、以下の条件のいずれかに該当する直線については、誤検出の可能性が高いとして、出力の対象外とする。
矩形領域座標取得処理(\ref{subsec:rect_coords_obtainment_processing}節を参照)の出力を、判定する条件の1つに用いる。

\begin{itemize}
    \item 直線の長さが10ピクセル未満である場合\\
        Canny法で適用するガウシアンフィルタで除去できていないノイズによって誤検出したエッジを下線部と捉えることを防ぐ。
    \item 水平を基準として傾きが3ピクセル以上である場合\\
        横書きの帳票において、下線部の直線は水平であるため、垂直な直線を下線部と捉えることを防ぐ。
    \item 直線が矩形領域の辺の一部から上下20ピクセル以内に存在する場合\\
        矩形領域座標取得処理の出力を判定に利用する。矩形領域の辺の一部を下線部と捉えることを防ぐ。
\end{itemize}

HoughLinesP関数によって直線を検出する際、入力画像内にある1本の直線の上下に、誤って2本の直線を検出してしまう不具合が発生する場合がある。
これは、両端点のxy座標が1ピクセル単位で異なる直線を、別の直線として検出するためである。
この不具合の発生を防ぐため、検出した直線の中点を全て計算し、ある直線における中点のy座標について、上下10ピクセル以内に別の直線の中点が存在する場合は、二直線の両端点のxy座標をそれぞれ平均して1本の直線に統一する。

\section{文字情報取得部}\label{sec:OCR_part}
文字情報取得部では、光学文字認識(\ref{sec:Optical-Charactor-Recognition}節を参照)によって、帳票画像内の文字情報を取得する。
本ツールでは、光学文字認識ソフトTesseract-OCR\cite{Tesseract-OCR}を用いる。
文字情報取得部の出力結果は、ラベル付与部(\ref{subsec:label_link_processing}節で後述)で用いる。

\subsection{文字情報取得用画像処理}\label{subsec:image_processing_for_char_recognition}
文字情報取得用画像処理は、この後の処理である文字情報取得処理(\ref{subsec:char_information_obtainment_processing}節で後述)を実行するための前処理として、文字の認識精度を高めるため、以下の順で帳票画像に画像処理を施す。

\begin{enumerate}
    \item DeblurGANv2の適用によるブレ除去後のグレースケール化帳票画像の生成\\
        \ref{subsec:rect_coords_obtainment_processing}節で述べた処理と同様の処理
    \item 領域座標取得部(\ref{sec:area_coords_obtainment_part}節を参照)から、領域座標を取得する。
    \item 背景色のRGBの値を取得\\
        背景色は、画像内で占める面積が最も広い色であるとする。
        全画素のRGB値を取得し、最も取得回数が多いRGB値を、背景色のRGB値とする。
    \item OpenCVのdrawContours関数(\ref{sec:OpenCV}節を参照)と、OpenCVのline関数(\ref{sec:OpenCV}節を参照)を用いた、背景色での矩形領域と下線部領域の描画\\
        矩形領域は矩形を、下線部領域は直線を、背景色で描画することによって、文字でない黒色の画素を減らす。
        これにより、文字でない黒色の画素値によって、大津の二値化で計算する閾値が変化することを防ぐ。
        本ツールでは、太さ15ピクセルの矩形と直線を描画する。
    \item OpenCVのimwrite関数を用いた、画像の保存\\
        背景色で矩形と直線を描画した画像を保存し、処理対象の画像を、入力である帳票画像から変更する。
    \item OpenCVのcvtColor関数を用いた、帳票画像のグレースケール化\\
        \ref{subsec:rect_coords_obtainment_processing}節で述べた処理と同様の処理
    \item OpenCVのthreshold関数を用いた、大津の二値化による二値画像への変換\\
        大津の二値化による閾値処理を行い、閾値の決定手法を指定する変数をTHRESH\_BINARYに指定し、二値化する。
\end{enumerate}

\subsection{文字情報取得処理}\label{subsec:char_information_obtainment_processing}
文字情報取得処理は、文字情報を取得する処理である。
\ref{subsec:image_processing_for_char_recognition}節の画像処理後、Tesseract-OCRによる文字認識を行う。
本ツールでは、PythonのOCR用のラッパーライブラリであるPyOCR\cite{PyOCR}から、変数builderにLineBoxBuilderを指定し、行単位で文字認識を行う。
これによって、取得文字と文字位置を行単位で取得することができる。

文字情報取得後、バウンディングボックスの左上頂点のy座標を参照し、昇順にソートし、番号を0から順に割り振る。
y座標が同じ場合は、さらにx座標を参照し、昇順にソートする。
本来は、y座標が同じ場合は、x座標を昇順にソートするため、左から右へ番号が大きくなる。
しかし、人間の目視で複数の文字が同じ行に存在すると認識するとき、割り振った番号を参照すると、昇順にならない不具合が発生する場合がある。
ラベル付与部(\ref{subsec:label_link_processing}節で後述)で文字位置を扱う際に、この不具合が起こった場合、ラベルの更新順が変化することで、異なるラベルを領域座標に割り付ける可能性がある。

ある帳票画像に対して文字位置取得処理を施し、同行に存在する文字列に対して、割り振った番号が昇順にならない場合の画像を、図\ref{fig:before_sorted_string}に示す。
図\ref{fig:before_sorted_string}内で描画している赤い矩形は、取得文字を囲むバウンディングボックスであり、バウンディングボックスの左上に、ソート後に割り振った番号を表示している。
図\ref{fig:before_sorted_string}の上部の文字に対して割り振った番号は、左から14番、15番、13番、12番となっている。
本来は、割り振った番号が左から12番、13番、14番、15番となる必要がある。

\begin{figure}[t]
    \begin{center}
        \fbox{
            \includegraphics[width=15cm]{image/04-implementation/before_sorted_string.png}
        }
        \caption{誤って番号を割り振った文字位置}
        \label{fig:before_sorted_string}
    \end{center}
\end{figure}

これは、Tesseract-OCRが文字を認識する順番を、y座標についてピクセル単位で昇順にソートするため、人間の目視で認識する順番とソート後の順番に違いが生じるためである。
以下に、この不具合の発生を防ぐため、再ソートを行う流れを示す。
以下の処理は、iを0に初期化し、文字を認識した順番で、取得文字の数だけ繰り返す。

\begin{enumerate}
    \item i番目のバウンディングボックスの左上頂点のy座標を取得する。
    \item 取得したy座標を基準として、10ピクセル以内に別のバウンディングボックスの左上頂点が存在する場合は、以下の処理を順に行う。
    \begin{enumerate}
        \item 条件にあてはまる左上頂点のxy座標全てを、空のリストgroupに格納する。
        \item リストgroup内について、x座標の値を参照して昇順にソートする。
        \item iをリストgroupの要素数だけ増やす。
        \item リストgroupの要素を全て削除する。
    \end{enumerate}
\end{enumerate}

これによって、人間の目視で同じ行に存在すると認識する複数の文字を対象に、文字位置取得時点のy座標について、最小のy座標と最大のy座標の差が10ピクセル以内であれば、正しくソートができる。
再ソート後にバウンディングボックスを描画した画像を、図\ref{fig:after_sorted_string}に示す。
ソート後は、図\ref{fig:before_sorted_string}では不具合が発生していた箇所が、図\ref{fig:after_sorted_string}では、左から12番、13番、14番、15番となっており、昇順となっていることがわかる。

\begin{figure}[t]
    \begin{center}
        \fbox{
            \includegraphics[width=15cm]{image/04-implementation/after_sorted_string.png}
        }
        \caption{再ソートによって昇順に並び替えた文字位置}
        \label{fig:after_sorted_string}
    \end{center}
\end{figure}

図\ref{fig:original}に示した帳票画像に対する、文字情報取得処理の出力をまとめたものを、図\ref{fig:char_recognition_for_original}に示す。
また、図\ref{fig:char_recognition_for_original}で示す文字位置を参照し、バウンディングボックスを描画した画像を、図\ref{fig:bbox_recognition_for_original}に示す。
図\ref{fig:char_recognition_for_original}で表示している内容を、図に示す。

% \begin{center}
%     \

% \begin{enumerate}
%     \item \ref{subsec:char_information_obtainment_processing}節で割り振った、ソート後の番号
%     \item バウンディングボックスの左上頂点のxy座標
%     \item バウンディングボックスの右下頂点のxy座標
%     \item 取得文字
% \end{enumerate}

例えば、番号1の取得文字は、「請求日」であり、バウンディングボックスの左上頂点のxy座標が、(1597, 321)であり、バウンディングボックスの右下頂点のxy座標が、(1724, 363)となる。
図\ref{fig:char_recognition_for_original}の番号は、図\ref{fig:bbox_recognition_for_original}のバウンディングボックスの左上に表示する番号と一致する。

\lstset{language=}
\begin{figure}[t]
    \begin{lstlisting}
    string[0] ((1644, 244), (1730, 285)) : 番号
    string[1] ((1597, 321), (1724, 363)) : 請求日
    string[2] ((1343, 486), (1411, 528)) : 詩
    string[3] ((1046, 518), (1106, 555)) : 月
    string[4] ((1194, 487), (1260, 555)) : 求
    string[5] ((1354, 530), (1400, 555)) : 較
    string[6] ((898, 683), (983, 725)) : 御中
    string[7] ((223, 992), (926, 1034)) : 下記の通り、ご請求申し上げます。
    string[8] ((1573, 1073), (1662, 1106)) : FAX
    string[9] ((229, 1147), (449, 1189)) : ご請求金額
    string[10] ((1126, 1147), (1248, 1190)) : (税込)
    string[11] ((1574, 1146), (1656, 1187)) : 担当
    string[12] ((1183, 1313), (1270, 1354)) : 数量
    string[13] ((1526, 1311), (1606, 1352)) : 単価
    string[14] ((1970, 1311), (2058, 1352)) : 合計
    string[15] ((1521, 2267), (1608, 2308)) : 小計
    string[16] ((1499, 2343), (1632, 2386)) : 消費税
    string[17] ((1520, 2422), (1608, 2463)) : 合計
    string[18] ((1160, 2576), (1293, 2619)) : 備考
    \end{lstlisting}
    \caption{図\ref{fig:original}から取得した文字}
    \label{fig:char_recognition_for_original}
\end{figure}

バウンディングボックスを描画していない文字については、認識できなかった文字である。
認識に失敗した文字が及ぼす影響については、\ref{sec:problems}節で後述する。

\begin{figure}[t]
    \begin{center}
        \fbox{
            \includegraphics[width=15cm]{image/04-implementation/char_with_bbox.png}
        }
        \caption{図\ref{fig:char_recognition_for_original}の文字位置を図\ref{fig:original}の画像に描画した画像}
        \label{fig:bbox_recognition_for_original}
    \end{center}
\end{figure}

\subsection{除外判定処理}\label{subsec:exclusion_judgement_processing}
除外判定処理は、Fugashi(\ref{sec:Fugashi}節を参照)による形態素解析を行い、属性推測処理(\ref{subsec:att_prediction_processing}節で後述)に不要な取得文字について、出力から除外する処理である。
形態素解析ソフトウェアであるMecab(\ref{sec:Fugashi}節を参照)を用いて、取得文字を形態素に分割し、品詞を解析する。
取得文字を構成する形態素の数のうち、特定の品詞である形態素数の割合が半分以上である場合は、属性判定において意味がない取得文字であるとして、該当の取得文字と文字位置を、文字情報から除外する。
文字を認識する際に、紙面と背景の境界や、領域取得部(\ref{sec:area_coords_obtainment_part}節を参照)で取得できなかった矩形や直線を、文字として誤認識する場合がある。
不要な文字の属性推測処理を防ぐことにより、処理時間を短縮することができる。

以下に、UniDic品詞体系(左からカンマ区切りで、大分類、中分類、小分類、細分類)をもとに、除外対象である形態素の品詞を示す。
なお、除外対象とする品詞は、経験から決定している。

\begin{itemize}
    \item 補助記号,一般,*,*
    \item 感動詞,一般,*,*
    \item 感動詞,フィラー,*,*
\end{itemize}

ある帳票画像に対して、\ref{subsec:char_information_obtainment_processing}節で取得したバウンディングボックスを描画した画像の一部を、図\ref{fig:before_exclusion_bbox}に示す。
また、図\ref{fig:before_exclusion_bbox}で描画したバウンディングボックスに対応する取得文字を出力した画像を、図\ref{fig:before_exclusion_string}に示す。
図\ref{fig:before_exclusion_bbox}と図\ref{fig:before_exclusion_string}の58番および60番の取得文字は、帳票画像内の矩形の辺を誤って文字として認識している。
この矩形については、文字情報取得用画像処理(\ref{subsec:image_processing_for_char_recognition}節を参照)で取得できなかったため、背景色の矩形を描画できていない。
図\ref{fig:before_exclusion_bbox}と図\ref{fig:before_exclusion_string}の58番および60番の取得文字は、属性推測処理において意味がない文字である。
図\ref{fig:before_exclusion_string}に示した取得文字に対して除外判定処理を適用し、属性推測処理に不要な取得文字を除外した出力を、図\ref{fig:after_exclusion_string}に示す。
図\ref{fig:after_exclusion_string}より、属性推測処理に不要な取得文字の除外に成功していることがわかる。

\begin{figure}[t]
    \begin{center}
        \fbox{
            \includegraphics[width=15cm]{image/04-implementation/before_exclusion_bbox.png}
        }
        \caption{属性判定に不要な文字を含む文字認識}
        \label{fig:before_exclusion_bbox}
    \end{center}
\end{figure}

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=15cm]{image/04-implementation/before_exclusion_string.png}
        \caption{除外判定処理適用前の取得文字}
        \label{fig:before_exclusion_string}
    \end{center}
\end{figure}

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=15cm]{image/04-implementation/after_exclusion_string.png}
        \caption{除外判定処理適用後の取得文字}
        \label{fig:after_exclusion_string}
    \end{center}
\end{figure}

\section{ラベル付与部}\label{sec:label_link_part}
ラベル付与部では、除外判定処理(\ref{subsec:exclusion_judgement_processing}節を参照)後の取得文字の属性を、属性の候補(\ref{subsec:att_prediction}節を参照)である、日付(date)、文字列(string)、数値(number)の3つから適切な1つを推測し、取得した領域にラベルとして付与する。
付与するラベルの種類は、領域近傍の取得文字から推測する属性に依存する。
領域座標と、領域座標に対応するラベルを組としたJSONファイルを出力とする。

\subsection{属性推測処理}\label{subsec:att_prediction_processing}
属性推測処理では、取得文字に対して、属性の候補である、日付(date)、文字列(string)、数値(number)の中から推測する。
なお、属性が推測不可である取得文字の属性は、文字列とする。
属性の推測には、大規模言語モデルYouri(\ref{sec:Youri}節を参照)を用いる。
YouriはLlama2を日本語の学習データで継続事前学習を行った大規模言語モデルである。
大規模言語モデルは、指示と異なる出力をする場合がある。
候補ではない属性の推測を防ぐため、Youriの出力から3つの属性のいずれかとなるよう補正を行う。
日本語の推論に特化した言語モデルを利用することによって、取得した日本語の文字に対して、属性をより正確に推測することができる。

以下に、属性を推測する処理の流れを示す。
以下の処理は、文字情報取得処理(\ref{subsec:char_information_obtainment_processing}節を参照)でソートを行った順番で、除外判定処理(\ref{subsec:exclusion_judgement_processing}節を参照)後の取得文字の数だけ繰り返す。

\begin{enumerate}
    \item 除外判定処理(\ref{subsec:exclusion_judgement_processing}節を参照)の出力である除外判定後の取得文字を受け取る。
    \item 以下のプロンプトを入力として属性を推測する。なお、(取得文字)は、除外判定後の取得文字を指す。\\
    \begin{quotation}
        \# 命令書:\\
        以下の制約条件にあてはまるものを出力せよ。
        
        \# 制約条件:\\
        ・記入欄に記入する内容が、日付、文字列、数値の中から、どのデータ型が最も適切であるかを選択する。\\
        ・出力は短く、あてはまるデータ型のみとする。\\
        ・例として、年月日などは日付、氏名などは文字列、金額などは数値があてはまる。\\
        (取得文字)という欄は、どのデータ型に該当するか。
    \end{quotation}
    \item 以下の順で処理を行い、属性を補正する。
        \begin{enumerate}
            \item 全文字の属性を文字列(string)とする。
            \item 出力に「日」を含む場合は、日付(date)として判定し、属性を文字列(string)から更新し、属性を補正する。
            \item 出力に「数」を含む場合は、数値(number)として判定し、属性を文字列(string)から更新し、属性を補正する。
            \item 出力に「日」、「数」を含まない場合は、属性を更新せず、文字列(string)とする。
        \end{enumerate}
\end{enumerate}

図\ref{fig:char_recognition_for_original}に示した取得文字に対して、Youriが出力したテキストの一部を、以下の図\ref{fig:output_Youri}に示す。
図\ref{fig:output_Youri}は、左にYouriの出力を、右の括弧内に属性を推測した取得文字を示す。
図\ref{fig:output_Youri}の1行目や4行目は、「番号」、「月」という取得文字に対して、属性の候補ではなく、文を出力している。

\lstset{language=}
\begin{figure}[t]
    \begin{lstlisting}
        数値に該当する。 (番号)
        日付 (請求日)
        文字列 (詩)
        文字列が適切です。 (月)
        日付 (求)
        数値 (較)
        日付 (御中)
        日付 (下記の通り、ご請求申し上げます。)
        数字です。 (FAX)
        数値 (ご請求金額)
        「(税込)」は、数値のデータ型に該当します。 ((税込))
        文字列 (担当)
        数値 (数量)
        数値型に該当する (単価)
        数値 (合計)
        数値 (小計)
        数値 (消費税)
        数値 (合計)
        備考は、文字列に該当する。 (備考)
    \end{lstlisting}
    \caption{図\ref{fig:char_recognition_for_original}に示した取得文字からYouriが出力したテキスト}
    \label{fig:output_Youri}
\end{figure}

図\ref{fig:output_Youri}で示したYouriの出力から、属性を補正した結果を、図\ref{fig:predict_att_for_original}に示す。
図\ref{fig:predict_att_for_original}は、左から、\ref{subsec:char_information_obtainment_processing}節で割り振った、ソート後の番号、属性を判定した結果、推測対象である取得文字の順で表す。
例えば、番号1の取得文字は、「請求日」であり、推測した属性は日付(date)となる。番号4の取得文字は、「月」であり、推測した属性は文字列(string)となる。
図\ref{fig:predict_att_for_original}の番号は、図\ref{fig:bbox_recognition_for_original}のバウンディングボックスの左上頂点に表示する番号、および図\ref{fig:char_recognition_for_original}で示した番号と一致する。
なお、大規模言語モデルによる出力に依存して属性を決定するため、常に正しい属性を判定することはできていない。
例えば、番号6の取得文字である「御中」の属性を、日付(date)と判定している。
御中は組織や団体の敬称であり、本来は文字列(string)の属性が正しい。

\lstset{language=}
\begin{figure}[t]
    \begin{lstlisting}
        att[0]: number (番号)
        att[1]: date (請求日)
        att[2]: string (詩)
        att[3]: date (月)
        att[4]: date (求)
        att[5]: number (較)
        att[6]: date (御中)
        att[7]: date (下記の通り、ご請求申し上げます。)
        att[8]: number (FAX)
        att[9]: number (ご請求金額)
        att[10]: number ((税込))
        att[11]: string (担当)
        att[12]: number (数量)
        att[13]: number (単価)
        att[14]: number (合計)
        att[15]: number (小計)
        att[16]: number (消費税)
        att[17]: number (合計)
        att[18]: string (備考)
    \end{lstlisting}
    \caption{図\ref{fig:original}に対して取得した文字情報}
    \label{fig:predict_att_for_original}
\end{figure}

\subsection{ラベル割付処理}\label{subsec:label_link_processing}
ラベル割付処理では、領域座標取得部(\ref{sec:area_coords_obtainment_part}節を参照)で取得した領域座標に対して、近傍に存在する文字の属性を割り付ける。
属性推測処理(\ref{subsec:att_prediction_processing}節を参照)で推測した属性と、文字情報取得処理(\ref{subsec:char_information_obtainment_processing}節を参照)で取得した文字位置をもとに、取得文字近傍の領域座標に対して、推測した属性をラベルとして割り付ける。

以下に、領域座標にラベルを割り付ける流れを示す。
以下の処理は、文字位置取得処理でソートを行った順番で、除外判定処理(\ref{subsec:exclusion_judgement_processing}節を参照)後の取得文字の数だけ繰り返す。

\begin{enumerate}
    \item \label{enum:bbox_center} 文字位置であるバウンディングボックスの中心点のxy座標を計算する。
    \item 取得した領域座標のうち、矩形領域は右下頂点のxy座標、下線部領域は右端点のxy座標と、計算した中心点のxy座標を比較し、\ref{enum:bbox_center}で計算した中心点のx座標とy座標が共に大きい全ての領域座標をラベル割付の対象として、文字位置に対応する取得文字の属性をラベルとして割り付ける。
    \item 繰り返し処理によって、既にラベルを割り付けた領域座標がラベル割付の対象となった場合は、後の処理で割り付けるラベルに更新する。
\end{enumerate}

以上の繰り返し処理を行った後、領域座標取得部(\ref{sec:area_coords_obtainment_part}節を参照)で取得した領域座標に対して、ラベルを割り付ける。

\section{ファイル出力部}\label{subsec:file_output_part}
ファイル出力部では、領域座標と、対応するラベルを組とするJSON形式のファイルと、取得領域を強調表示したPNG形式の画像2枚を出力する。
JSONファイルと、PNG画像のエクスポートにあたって、領域座標取得部(\ref{sec:area_coords_obtainment_part}節を参照)で取得した領域座標と、ラベル割付処理(\ref{subsec:label_link_processing}節を参照)で取得したラベルを参照する。


\subsection{JSONファイル出力処理}\label{subsec:json_file_output_processing}
JSONファイル出力処理では、取得した領域座標とラベルを整形し、領域座標と、対応するラベルを組とするJSONファイルを出力する処理である。
出力するJSONファイルは、配列rects\_dataと、配列underlines\_dataで構成し、矩形領域の情報、下線部領域の情報をそれぞれ持つ。
これら2つの配列は、それぞれ以下の情報をもつ。

\begin{itemize}
    \item 配列ごとに一意であるid
    \item 領域に割り付けているラベルを示すlabel
    \item 領域座標を示すオブジェクトcoords
\end{itemize}

配列rects\_dataと、配列underlines\_dataを、図\ref{fig:example_output_json}に示す形式に整形後、JSONファイルを出力する。

\subsection{領域強調画像出力処理}\label{subsec:area_highlighted_image_output_processing}
領域強調画像出力処理では、領域座標取得部(\ref{sec:area_coords_obtainment_part}節を参照)で取得した領域座標と、ラベル割付処理(\ref{subsec:label_link_processing}節を参照)で取得したラベルを参照し、矩形と直線を、割り付けたラベルと共に描画した画像を出力する。
出力する画像については、矩形領域強調画像と、下線部領域強調画像の計2枚の画像を出力する。

矩形領域強調画像については、矩形をランダムなRGBカラーで描画することで、矩形領域を強調表示する。
さらに、左上頂点に、JSONファイル内のrect\_data配列のidキーに対応する値と、labelキーに対応する値を表示する。
下線部の帳票画像記入欄については、緑色で下線部領域の直線を描画することによって、下線部領域を強調表示する。
さらに、左端点の上に、JSONファイル内のunderlines\_data配列のidキーに対応する値と、labelキーに対応する値を表示する。
これにより、人間が目視でJSONファイルを確認する場合と比較して、取得した領域座標と割り付けたラベルの確認が容易となる。

以下に、矩形と直線を、割り付けたラベルと共に描画した画像を出力する流れを示す。

\begin{enumerate}
    \item \ref{sec:area_coords_obtainment_part}節と\ref{subsec:label_link_processing}節から、領域座標と、領域座標に対応するラベルを、それぞれ取得する。
    \item Pythonのcopy関数により、入力である帳票画像をコピーし、2枚の帳票画像A、Bを生成する。
    \item 帳票画像Aに対して、OpenCVのdrawContours関数を用いて、矩形領域座標を参照して矩形を描画する。
    \item 帳票画像Bに対して、OpenCVのline関数を用いて、下線部領域座標を参照して直線を描画する。
    \item OpenCVのputText関数(\ref{sec:OpenCV}節を参照)を用いて、idキーに対応する値と、labelキーに対応する値を、各領域座標の左上に描画する。
    \item OpenCVのimwrite関数を用いて、帳票画像Aと帳票画像Bを、それぞれ矩形領域強調画像と下線部領域強調画像として保存する。
\end{enumerate}

以上の処理後、矩形領域強調画像と、下線部領域強調画像を出力する。